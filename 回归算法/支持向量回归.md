<h2>SVR</h2>

<h3>介绍</h3>

　　SVR（支持向量回归）是使用支持向量机解决回归问题。支持向量回归假设我们能容忍的f(x)与之间最多有ε的偏差，当且仅当f(x)与y的差别绝对值大于ε时，才计算损失，此时相当于以f(x)为中心，构建一个宽度为2ε的间隔带，若训练样本落入此间隔带，则认为是被预测正确的。

<h3>理论知识</h3>

1.基本原理

　　支持向量机(SVM)本身是针对二分类问题提出的，而SVR（支持向量回归）是SVM（支持向量机）中的一个重要的应用分支。SVR回归与SVM分类的区别在于，SVR的样本点最终只有一类，它所寻求的最优超平面不是SVM那样使两类或多类样本点分的“最开”，而是使所有的样本点离着超平面的总偏差最小。

    SVM是要使到超平面最近的样本点的“距离”最大；

    SVR则是要使到超平面最远的样本点的“距离”最小。

2.支持向量回归的推导

> 给定训练样本D={(x1,y1),(x1,y1),……，(xm,ym)},yi∈R,希望学得一个回归模型,使得f(x)与y尽可能接近,w和b是待确定的模型参数。

> 假设我们能容忍f(x)与y之间最多有ϵ的偏差,即仅当f(x)与y之间的差别绝对值大于ϵ时才计算损失.于是，SVR问题可形式化为:![image](/uploads/1c95812f7420a9e9be80cfe70b48b956/image.png)

> 其中C为正则化常数,lϵ是ϵ-不敏感损失(ϵ -insensitive loss)函数:![image](/uploads/73460b5ef0bd7bf5e408029a733f90c0/image.png)

> 引入松弛变量ξi和(ξi),可将式重写为:
![image](/uploads/30ee7be8d899b56553a3f1016039e04f/image.png)

> 引入拉格朗日乘子μi，再令![image](/uploads/615d08fe4fc257bb51ca7827a8dd530d/image.png)对w,b,ξi和![image](/uploads/470edfa39063f2a8263afb54abe37446/image.png)的偏导为零可得:![image](/uploads/812a2b0c19a314bf29c3fa144bef8f98/image.png)
​	

> 上述过程中需满足KKT条件，即要求：![image](/uploads/3dd620bd1ec88191557c0a52b6decb0f/image.png)

> SVR的解形如:![image](/uploads/c7e7a0463c82385bae111e368e6cffd7/image.png)

> 能使式中的(αiˆ−αi)≠0的样本即为SVR的支持向量,它付必落在ϵ-同隔带之外.显然, SVR的支持向量仅是训练样本的一部分,即其解仍具有稀疏性.若0<αi<C，则必有ξi=0,![image](/uploads/7b2c48dda33f7bcd25fa7d64fe4fb2a4/image.png)实践中常采用一中更鲁棒的办法:迭取多个满足条件0<αi<C的样本求解b后取平均値。
	
> 若考虑特征映射形式,则：![image](/uploads/2f115090a9af24c05c6165233f069f55/image.png)

> 则SVR可表示为：![image](/uploads/cd7a505ca86c816ce921b9b966a83b03/image.png)其中,![image](/uploads/810582211160536a14e3801476f32c83/image.png)为核函数。

<h3>python/pyspark样例代码</h3>

> 1.导入：`from sklearn.svm import SVR`;

> 2.创建模型:`model=SVR(kernel=kernel, degree=int(degree), gamma=gamma, coef0=coef0, tol=tol, C=C, epsilon=epsilon, shrinking=shrinking, cache_size=cache_size, max_iter=max_iter)`

> 3.训练：`svrModel = model.fit(X_train, y_train)`

> 4.预测：`y_pre = model.predict(X_test)` 

<h3>参数说明</h3>

> kernel：核函数,指定要在算法中使用的内核类型，可选：rbf（高斯核函数）、linear（线性核函数）、poly（多项式核函数）、sigmoid（sigmoid核函数），默认rbf。

> degree: 多项式阶数,正整数。该参数仅在内核为多项式核函数时起作用。默认2。 

> gamma: 核系数,正float类型或auto。该参数仅在内核为多项式、高斯或sigmoid核函数时起作用。auto表示没有传递明确的gamma值。

> coef0: 独立项,正float类型。该参数仅在内核为多项式或sigmoid核函数时起作用。默认0.0。

> tol: 容错率,(0,1)之间的float类型。模型停止训练的容错标准。默认1e-3。

> C: 惩罚系数,正float类型。错误的惩罚系数。默认1.0。

> epsilon: 距离误差：float类型。训练集中的样本需满足模型拟合值与实际值的误差。默认0.1。

> shrinking: 收缩启发式,是否使用收缩启发式。（True、False）。默认True。

> cache_size: 缓存大小（MB）,float类型。当数据较大时，指定内核缓存的大小。（以MB为单位）。默认200MB。

> max_iter:最大迭代次数：int类型。进行迭代次数的上限。-1为无限制。默认-1。

<h3>适用场景</h3>

　　支持向量回归算法主要应用场景有预测控制、图像识别、金融时间序列预测、模式识别、水质预测等。
