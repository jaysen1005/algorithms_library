<h2>SVR</h2>

<h3>介绍</h3>

　　SVR（支持向量回归）是使用支持向量机解决回归问题。支持向量回归假设我们能容忍的f(x)与之间最多有ε的偏差，当且仅当f(x)与y的差别绝对值大于ε时，才计算损失，此时相当于以f(x)为中心，构建一个宽度为2ε的间隔带，若训练样本落入此间隔带，则认为是被预测正确的。

<h3>理论知识</h3>

1.基本原理

　　CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。

　　CART算法由以下两步组成：

　　（1）决策树的生成：基于训练数据集生成决策树，生成的决策树要尽量大（大是为了更好地泛化）

　　（2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。

2.回归树的生成

　　 在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。

> （1）选择最优切分变量j与切分点s，求解![image](/uploads/d418df0f328a9e541528b4b7b746ca5d/image.png)

> 遍历变量j，对固定的切分变量j扫描切分点s，选择使上式达到最小值的对(j,s)j。

> （2）用选定的对(j,s)划分区域并决定响应的输出值：![image](/uploads/cfcf1db60e31185d974e7a427c63515a/image.png)

> （3）继续对两个子区域条用步骤（1），（2），直至满足停止条件。

> （4）将输入空间划分为M个区域R1,R2,...,RM,生成决策树：![image](/uploads/4576f9bd1c6bc57c0fe693912aed6b39/image.png)

<h3>简单示例</h3>

> 1.导入：`from sklearn.svm import SVR`;

> 2.创建模型:`model=SVR(kernel=kernel, degree=int(degree), gamma=gamma, coef0=coef0, tol=tol, C=C, epsilon=epsilon, shrinking=shrinking, cache_size=cache_size, max_iter=max_iter)`

> 3.训练：`svrModel = model.fit(X_train, y_train)`

> 4.预测：`y_pre = model.predict(X_test)` 

<h3>参数说明</h3>

> kernel：核函数,指定要在算法中使用的内核类型，可选：rbf（高斯核函数）、linear（线性核函数）、poly（多项式核函数）、sigmoid（sigmoid核函数），默认rbf。

> degree: 多项式阶数,正整数。该参数仅在内核为多项式核函数时起作用。默认2。 

> gamma: 核系数,正float类型或auto。该参数仅在内核为多项式、高斯或sigmoid核函数时起作用。auto表示没有传递明确的gamma值。

> coef0: 独立项,正float类型。该参数仅在内核为多项式或sigmoid核函数时起作用。默认0.0。

> tol: 容错率,(0,1)之间的float类型。模型停止训练的容错标准。默认1e-3。

> C: 惩罚系数,正float类型。错误的惩罚系数。默认1.0。

> epsilon: 距离误差：float类型。训练集中的样本需满足模型拟合值与实际值的误差。默认0.1。

> shrinking: 收缩启发式,是否使用收缩启发式。（True、False）。默认True。

> cache_size: 缓存大小（MB）,float类型。当数据较大时，指定内核缓存的大小。（以MB为单位）。默认200MB。

> max_iter:最大迭代次数：int类型。进行迭代次数的上限。-1为无限制。默认-1。

<h3>适用场景</h3>

　　支持向量回归树算法适用于稀疏数据拟合，能够很好的解释结果。