<h2>KNN回归</h2>

<h3>介绍</h3>

　　KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的某个（些）属性的平均值赋给该样本，就可以得到该样本对应属性的值。

<h3>理论知识</h3>

1.KNN回归算法关键：

　　(1)数据的所有特征都要做可比较的量化
　　若是数据特征中存在非数值的类型，必须采取手段将其量化为数值。例如样本特征中包含颜色，可通过将颜色转换为灰度值来实现距离计算。
　　(2)样本特征要做归一化处理
　　样本有多个参数，每一个参数都有自己的定义域和取值范围，他们对距离计算的影响不一样，如取值较大的影响力会盖过取值较小的参数。所以样本参数必须做一些scale处理，最简单的方式就是所有特征的数值都采取归一化处理。
归一化方法有：

>min-max标准化（Min-Max Normalization）：![b1]　　　　(/uploads/0c7ce23fc218b413c46b041f4c421cea/b1.png)

>Z-score标准化（Z-score standardization）：![b2](/uploads/cadd54b56d4b71b2a9b9bc07f876b39c/b2.png)，其中 μ 为所有样本数据的均值，σ为所有样本数据的标准差。

　　(3)需要一个距离函数以计算两个样本之间的距离
　　距离的定义：欧氏距离、余弦距离、汉明距离、曼哈顿距离等，一般选欧氏距离作为距离度量，但是这是只适用于连续变量。在文本分类这种非连续变量情况下，汉明距离可以用来作为度量。通常情况下，如果运用一些特殊的算法来计算度量的话，K近邻分类精度可显著提高，如运用大边缘最近邻法或者近邻成分分析法。
具体计算公式如下：
　　>1.欧式距离： 即两点间的空间距离，为两点向量差的L2范数。两个n维向量A（x11,x12,...,x1n）和B（x21，x22,...,x2n）间的欧氏距离为：![d2](/uploads/c4e696487d4e0a96ba68b276e452c96f/d2.gif)

　　>2.余弦距离：余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。公式如下：![d5](/uploads/8c66a695939671d3ac68d324654182e3/d5.png)

　　>3.汉明距离：汉明距离定义：两个等长字符串s1和s2之间的汉明距离定义为将其中一个变为另外一个所需要的最小替换次数。如字符串“1111”与“1001”之间的汉明距离为2。也就是指两个字符串的接近程度。

　　>4.曼哈顿距离：即城市街区距离，为两点向量差的L1范数。两个n维向量A（x11,x12,...,x1n）和B（x21，x22,...,x2n）间的曼哈顿距离为：![d1](/uploads/bf5bc05dffca965ebeabd6cb3f2d80c8/d1.gif)

　　>5.闵可夫斯基距离：相对于Lp范数，是一组距离的定义。两个n维向量A（x11,x12,...,x1n）和B（x21，x22,...,x2n）间的闵可夫斯基距离的定义为：![d4](/uploads/4240874fbbe731e0e3ec4de2465b8a54/d4.gif)

　　>6.切比雪夫距离：即最大的维度内距离，为两点向量差的无穷范数。两个n维向量A（x11,x12,...,x1n）和B（x21，x22,...,x2n）间的切比雪夫距离为：![d3](/uploads/08027f2beb5f8817d46a242df5e19b1f/d3.gif)
或另一种等价形式：![d3_1](/uploads/dd25c45e841a9386244b840cba4b65a6/d3_1.gif)

(4)确定K的值
　　K值选的太大易引起欠拟合，太小容易过拟合。交叉验证确定K值。

4.算法过程

> 1：计算测试数据与各个训练数据之间的距离；

> 2：按照距离的递增关系进行排序

> 3：确定前K个点所在类别的出现频率；

> 4：确定前K个点所在类别的出现频率； 

> 5：返回前K个点中出现频率最高的类别作为测试数据的预测分类。

<h3>简单示例</h3>

> 1.导入：`from sklearn.neighbors import KNeighborsRegressor`;

> 2.创建模型:`knnclf=KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm, leaf_size=leaf_size, metric=metric, p=p,n_jobs=1,metric_params=None)`

> 3.训练：`knnclf.fit(X_train,y_train)`

> 4.预测：`y_pre = knnclf.predict(x_test)`

<h3>参数说明</h3>

> n_neighbors：int, 可选参数(默认为 5),用于kneighbors查询的默认邻居的数量。

> weights：距离的权重；uniform：一致的权重；distance：距离的倒数作为权重

> algorithm：算法，可选值：'{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}'，快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索，brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。ball tree是为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。。

> leaf_size：int，optional(默认值= 30),这个是构造的kd树和ball树的大小。这个值的设置会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需要根据问题的性质选择最优的大小。

> metric：字符串或可调用，用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。 

> p：闵可斯基距离的p值; p=1:即欧式距离；p=2:即曼哈顿距离；p取1-6测试

> n_jobs：int或None，可选(默认=None)。并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。。

> metric_params：dict，optional(默认=None)，距离公式的其他关键参数，这个可以不管，使用默认的None即可。

<h3>适用场景</h3>

　　kmeans算法是一个比较通用的算法，适用于凸样本集，比如均匀的簇大小, 平面几何, 不是太多的簇等。