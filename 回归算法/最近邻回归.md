<h2>KNN回归</h2>

<h3>介绍</h3>

　　KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的某个（些）属性的平均值赋给该样本，就可以得到该样本对应属性的值。

<h3>理论知识</h3>

1.KNN回归算法的实现过程如下：

1）计算测试数据与各个训练数据之间的距离； 
2）按照距离的递增关系进行排序； 
3）选取距离最小的K个点； 
4）确定前K个点所在类别的出现频率； 
5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。

算法关键：
(1)数据的所有特征都要做可比较的量化
若是数据特征中存在非数值的类型，必须采取手段将其量化为数值。例如样本特征中包含颜色，可通过将颜色转换为灰度值来实现距离计算。
(2)样本特征要做归一化处理
样本有多个参数，每一个参数都有自己的定义域和取值范围，他们对距离计算的影响不一样，如取值较大的影响力会盖过取值较小的参数。所以样本参数必须做一些scale处理，最简单的方式就是所有特征的数值都采取归一化处理。
归一化方法有：

>min-max标准化（Min-Max Normalization）：![b1](/uploads/0c7ce23fc218b413c46b041f4c421cea/b1.png)

>Z-score标准化（Z-score standardization）：![b2](/uploads/cadd54b56d4b71b2a9b9bc07f876b39c/b2.png)，其中 μ 为所有样本数据的均值，σ为所有样本数据的标准差。

(3)需要一个距离函数以计算两个样本之间的距离
距离的定义：欧氏距离、余弦距离、汉明距离、曼哈顿距离等，一般选欧氏距离作为距离度量，但是这是只适用于连续变量。在文本分类这种非连续变量情况下，汉明距离可以用来作为度量。通常情况下，如果运用一些特殊的算法来计算度量的话，K近邻分类精度可显著提高，如运用大边缘最近邻法或者近邻成分分析法。
具体计算公式如下：
>1.欧式距离： 即两点间的空间距离，为两点向量差的L2范数。两个n维向量A（x11,x12,...,x1n）和B（x21，x22,...,x2n）间的欧氏距离为：![d2](/uploads/c4e696487d4e0a96ba68b276e452c96f/d2.gif)

>2.余弦距离：余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。公式如下：![d5](/uploads/8c66a695939671d3ac68d324654182e3/d5.png)

>3.汉明距离：汉明距离定义：两个等长字符串s1和s2之间的汉明距离定义为将其中一个变为另外一个所需要的最小替换次数。如字符串“1111”与“1001”之间的汉明距离为2。也就是指两个字符串的接近程度。

>4.曼哈顿距离：即城市街区距离，为两点向量差的L1范数。两个n维向量A（x11,x12,...,x1n）和B（x21，x22,...,x2n）间的曼哈顿距离为：![d1](/uploads/bf5bc05dffca965ebeabd6cb3f2d80c8/d1.gif)

>5.闵可夫斯基距离：相对于Lp范数，是一组距离的定义。两个n维向量A（x11,x12,...,x1n）和B（x21，x22,...,x2n）间的闵可夫斯基距离的定义为：![d4](/uploads/4240874fbbe731e0e3ec4de2465b8a54/d4.gif)

>6.切比雪夫距离：即最大的维度内距离，为两点向量差的无穷范数。两个n维向量A（x11,x12,...,x1n）和B（x21，x22,...,x2n）间的切比雪夫距离为：![d3](/uploads/08027f2beb5f8817d46a242df5e19b1f/d3.gif)
或另一种等价形式：![d3_1](/uploads/dd25c45e841a9386244b840cba4b65a6/d3_1.gif)

(4)确定K的值
K值选的太大易引起欠拟合，太小容易过拟合。交叉验证确定K值。

2.距离的度量 

　　距离的度量方法主要分为以下几种：

　　　　*  a.闵可夫斯基距离(Minkowski distance)：

　　　　![image](/uploads/d618baa5b3766e465cf9227af1aecbf7/image.png)

　　　　*  b.欧氏距离(Euclidean distance)，即当 p=2 时的闵可夫斯基距离:

　　　　![image](/uploads/7eda6b0012384d7c6b5543cb5cc2b208/image.png)

　　　　*  c.曼哈顿距离(Manhattan distance)，即当 p=1 时的闵可夫斯基距离:

　　　　![image](/uploads/5abe462995bfaead2d96d56d54854e0d/image.png)

3.更新簇中心

　　对于划分好的各个簇，计算各个簇中的样本点均值，将其均值作为新的簇中心。

4.算法过程

> 1：计算测试数据与各个训练数据之间的距离；

> 2：按照距离的递增关系进行排序

> 3：确定前K个点所在类别的出现频率；

> 4：确定前K个点所在类别的出现频率； 

> 5：返回前K个点中出现频率最高的类别作为测试数据的预测分类。

<h3>简单示例</h3>

> 1.导入：`from sklearn.neighbors import KNeighborsRegressor`;

> 2.创建模型:`knnclf=KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm, leaf_size=leaf_size, metric=metric, p=p,n_jobs=1,metric_params=None)`

> 3.训练：`knnclf.fit(X_train,y_train)`

> 4.预测：`y_pre = knnclf.predict(x_test)`

<h3>参数说明</h3>

> n_neighbors：int, 可选参数(默认为 5),用于kneighbors查询的默认邻居的数量。

> weights：距离的权重；uniform：一致的权重；distance：距离的倒数作为权重

> leaf_size：初始值选择方式，可选值：'k-means++'（用均值）、'random'（随机）、an ndarray（指定一个数组），默认为'k-means++'。

> metric：聚类数，模型聚类结果的类别数目，大于0的整数。 

> p：闵可斯基距离的p值; p=1:即欧式距离；p=2:即曼哈顿距离；p取1-6测试

> n_jobs：初始值选择方式，可选值：'k-means++'（用均值）、'random'（随机）、an ndarray（指定一个数组），默认为'k-means++'。



> random_state:随机种子数。

<h3>适用场景</h3>

　　kmeans算法是一个比较通用的算法，适用于凸样本集，比如均匀的簇大小, 平面几何, 不是太多的簇等。