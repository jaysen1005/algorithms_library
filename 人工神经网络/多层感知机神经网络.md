<h3>多层感知机神经网络</h3>

##### 简介

  多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构，如下图：![n](/uploads/ff4aa3957282df1e46438536c0407c9d/n.png)

从上图可以看到，多层感知机层与层之间是全连接的。多层感知机最底层是输入层，中间是隐藏层，最后是输出层。假设输入层用向量X表示，则隐藏层的输出就是f(W1X+b1)，W1是权重（也叫连接系数），b1是偏置，函数f可以是常用的sigmoid函数或者tanh函数：![n1](/uploads/93d74f97e639f0366324d35c6e292f89/n1.png)。
隐藏层到输出层可以看成是一个多类别的逻辑回归，也即softmax回归，所以输出层的输出就是softmax(W2X1+b2)，X1表示隐藏层的输出f(W1X+b1)。上面说的这个三层的MLP用公式总结起来就是函数G：![n2](/uploads/56ae4e4a7727dd3574384fae31ec5a70/n2.png)。MLP所有的参数就是各个层之间的连接权重以及偏置，包括W1、b1、W2、b2。

##### 理论知识

神经网络要解决的最基本问题是分类问题。我们将特征值从输入层传入隐藏层中，通过带有结果的数据来训练神经网络的参数(W，权重；b，偏置)，使输出值与我们给出的结果一致，既可以用来预测新的输入值了。神经网络预测首先要训练网络，通过训练使网络具有联想记忆和预测能力，具体步骤如下：
>（1）网络初始化
假设输入层的节点个数为，隐含层的节点个数为，输出层的节点个数为。输入层到隐含层的权重，隐含层到输出层的权重为，输入层到隐含层的偏置为，隐含层到输出层的偏置为。学习速率为，激励函数为。其中激励函数为取Sigmoid函数。形式为：![n3](/uploads/15b18df43bd68e97ad53f7c8ef2c45ee/n3.png)。

>（2）隐含层的输出
根据输入变量x，输入层和隐含层间连接权值w以及隐含层阈值a，计算隐含层的输出![n4](/uploads/66315ab82be00b89bb07e20b05113412/n4.png)为：![n4_1](/uploads/028893e6cfeb63d6001d1b197d7a2f1c/n4_1.png)。：

>（3）输出层的输出
根据隐含层输出H，连接权值w和阈值b，计算输出为：![n5](/uploads/f3d0bbe7b63bc287ba99d850976eb5ba/n5.png)。

>（4）根据网络预测输出O和期望输出Y，计算网络预测误差：![n6](/uploads/7582855c3ff85c62bdca0e026cb05c1c/n6.png)。其中![n6_4](/uploads/a970accad33ddd51c8880354d1984233/n6_4.png)为期望输出。我们记![n6_5](/uploads/008e955d3df51d0a9e1449d9f9fb2bac/n6_5.png)，则可以表示为![n6_2](/uploads/ee65e032cf2473ae16e3af47a0c51d39/n6_2.png)，以上公式中，![n6_3](/uploads/64372a4fce3c7ef374c0dc73a8580853/n6_3.png)。

>（5）权值的更新                          
权值的更新公式为：![n6_6](/uploads/34e0813cd8ecf38dfc7299f565bc6e05/n6_6.png)。

>（6）阈值的更新                                    
阈值的更新公式为：![n7](/uploads/64399b1a3ea056ef55cfbd4cde8ae117/n7.png)。
隐含层到输出层的阈值更新：![n7_1](/uploads/edfb7fa6a26c109e8f6dcdb8964fccb0/n7_1.png)，则阈值的更新公式为：![n7_2](/uploads/88d15d1ca89c5bdcc5bb50a1910a0612/n7_2.png)；输入层到隐含层的阈值更新：![n7_3](/uploads/768551c2348e78dae55af5250db33df2/n7_3.png)，其中![n7_4](/uploads/adf2a8539768ed558fb90f54f7b546db/n7_4.png)，![n7_5](/uploads/7a72555dc480cc208ce74a24f4f28f03/n7_5.png)，则偏置的更新公式为：![n7_6](/uploads/09b8a56934f1aca784148cf47c29be33/n7_6.png)。

>（7）判断算法迭代是否结束
有很多的方法可以判断算法是否已经收敛，常见的有指定迭代的代数，判断相邻的两次误差之间的差别是否小于指定的值等等。

##### 应用场景

由于MLP预测准确性较高，适应性强，最适合用于解决分类问题，在金融领域运用最多，如银行或企业破产分类预测、债券信用等级分类、商业贷款信用风险分类、股票的分类与选择等，在人力资源方面可用于员工绩效评价，营销管理方面进行目标顾客细分、目标市场细分等。

##### 参数及说明

`class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)`

*  **参数**

>**hidden_​​layer_sizes：**tuple，length = n_layers - 2，默认值（100，）第i个元素表示第i个隐藏层中的神经元数量。

>**activation：**{‘identity’，‘logistic’，‘tanh’，‘relu’}，默认’relu’。隐藏层的激活函数：‘identity’，无操作激活，对实现线性瓶颈很有用，返回f（x）= x；‘logistic’，logistic sigmoid函数，返回f（x）= 1 /（1 + exp（-x））；‘tanh’，双曲tan函数，返回f（x）= tanh（x）；‘relu’，整流后的线性单位函数，返回f（x）= max（0，x）。

>**slover：**{‘lbfgs’，‘sgd’，‘adam’}，默认’adam’。权重优化的求解器：'lbfgs’是准牛顿方法族的优化器；'sgd’指的是随机梯度下降。'adam’是指由Kingma，Diederik和Jimmy Ba提出的基于随机梯度的优化器。注意：默认解算器“adam”在相对较大的数据集（包含数千个训练样本或更多）方面在训练时间和验证分数方面都能很好地工作。但是，对于小型数据集，“lbfgs”可以更快地收敛并且表现更好。

>**alpha：**float，可选，默认为0.0001。L2惩罚（正则化项）参数。

>**batch_size：**int，optional，默认’auto’。用于随机优化器的minibatch的大小。如果slover是’lbfgs’，则分类器将不使用minibatch。设置为“auto”时，batch_size = min（200，n_samples）。

>**learning_rate：**{‘constant’，‘invscaling’，‘adaptive’}，默认’constant’（常数）。 用于权重更新。仅在solver ='sgd’时使用。
  *  'constant’：是’learning_rate_init’给出的恒定学习率；
  *  'invscaling’：使用’power_t’的逆缩放指数在每个时间步’t’逐渐降低学习速率learning_rate_， effective_learning_rate = learning_rate_init / pow（t，power_t）；只要训练损失不断减少，
  *  ‘adaptive’（自适应）：将学习速率保持为“learning_rate_init”。每当两个连续的时期未能将训练损失减少至少tol，或者如果’early_stopping’开启则未能将验证分数增加至少tol，则将当前学习速率除以5。

>**power_t：**double，可选，默认为0.5。反缩放学习率的指数。当learning_rate设置为“invscaling”时，它用于更新有效学习率。仅在solver ='sgd’时使用。

>**max_iter：**int，optional，默认值200。最大迭代次数。solver迭代直到收敛（由’tol’确定）或这个迭代次数。对于随机解算器（‘sgd’，‘adam’），请注意，这决定了时期的数量（每个数据点的使用次数），而不是梯度步数。

>**shuffle：**bool，可选，默认为True。仅在solver ='sgd’或’adam’时使用。是否在每次迭代中对样本进行洗牌。

>**random_state：**int，RandomState实例或None，可选，默认无随机数生成器的状态或种子。如果是int，则random_state是随机数生成器使用的种子;如果是RandomState实例，则random_state是随机数生成器;如果为None，则随机数生成器是np.random使用的RandomState实例。

>**tol：**float，optional，默认1e-4 优化的容忍度，容差优化。当n_iter_no_change连续迭代的损失或分数没有提高至少tol时，除非将learning_rate设置为’adaptive’，否则认为会达到收敛并且训练停止。

>**verbose：**bool，可选，默认为False 是否将进度消息打印到stdout。

>**warm_start：**bool，可选，默认为False，设置为True时，重用上一次调用的解决方案以适合初始化，否则，只需擦除以前的解决方案。请参阅词汇表。

>**momentum：**float，默认0.9，梯度下降更新的动量。应该在0和1之间。仅在solver ='sgd’时使用。

>**nesterovs_momentum：**布尔值，默认为True。是否使用Nesterov的势头。仅在solver ='sgd’和momentum> 0时使用。

>**early_stopping：**bool，默认为False。当验证评分没有改善时，是否使用提前停止来终止培训。如果设置为true，它将自动留出10％的训练数据作为验证，并在验证得分没有改善至少为n_iter_no_change连续时期的tol时终止训练。仅在solver ='sgd’或’adam’时有效。

>**validation_fraction：**float，optional，默认值为0.1。将训练数据的比例留作早期停止的验证集。必须介于0和1之间。仅在early_stopping为True时使用。

>**beta_1：**float，optional，默认值为0.9，估计一阶矩向量的指数衰减率应为[0,1)。仅在solver ='adam’时使用。

>**beta_2：**float，可选，默认为0.999,估计一阶矩向量的指数衰减率应为[0,1)。仅在solver ='adam’时使用。

>**epsilon：**float，optional，默认值1e-8, adam稳定性的价值。 仅在solver ='adam’时使用。

>**n_iter_no_change：**int，optional，默认值10,不符合改进的最大历元数。 仅在solver ='sgd’或’adam’时有效。

*  **方法**

>fit（X，y）：使模型适合数据矩阵X和目标y。

>get_params（[deep]）：获取此估算器的参数。

>predict（X）：使用多层感知器分类器进行预测。

>predict_log_proba（X）：返回概率估计的对数。

>predict_proba（X）：概率估计。

>score（X，y [，sample_weight]）：返回给定测试数据和标签的平均准确度。

>set_params（** params）：设置此估算器的参数。

##### **python/pyspark**样例代码

`from sklearn.neural_network import MLPClassifier`

`X = [``[0., 0.], [1., 1.]``]`

`y = [0, 1]`

`clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1)`

`clf.fit(X, y)`

`clf.predict([``[2., 2.], [-1., -2.]``])`

`clf.predict_proba([``[0.9]``])`