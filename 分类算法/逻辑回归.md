

<h3>逻辑回归</h3>

##### 简介

  逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。简单来说，它就是通过拟合一个逻辑函数（logit fuction）来预测一个事件发生的概率。所以它预测的是一个概率值，自然，它的输出值应该在0到1之间。

##### 理论知识

Logistic Regression可以简单的描述为这样的过程：

（1）找一个合适的预测函数，一般表示为h函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。逻辑回归的函数形式为：![h1](/uploads/5aa048bb08ffd705704fb9d176fe87d2/h1.png)。Z是线性变换，线性变换经过某种转换关系可以更加接近真实值Y的预测值，这里的转换关系为：![h2](/uploads/98569f6ba367380573748bf7c0e02ab8/h2.png)，称为sigmoid函数。

（2）构造一个Cost函数（损失函数），该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。假设训练样本共有 m 个 ，则上标 i 表示第i个样本，对于训练样本，我们希望预测的输出结果尽可能的接近真实的结果，即使![h3](/uploads/a0951c7e5d1421944f1bee6f31077f1c/h3.png)。
逻辑回归的损失函数为：![h4](/uploads/1e05534c54e9cec15243b99c915b006e/h4.png)，定义代价函数为 m 个训练样本损失函数的平均值：![h5](/uploads/55c83616e57839ba136e752c46740281/h5.png)。它衡量了预测结果与真实结果之间的平均错误代价，优化的目标是最小化代价函数J（w,b）。

（3）显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。梯度下降是神经网络中训练模型最常用的一种优化方法，梯度下降中，w,b 的更新方式为：![h6](/uploads/907c5f615e19c366d7ab27309b02cd76/h6.png)。alpha为学习率learning-rate表示移动步长，梯度（dw,db，即当前点的斜率)指定了移动方向，梯度下降法是为了寻找极小值，因此是朝梯度的负方向移动。

##### 应用场景

Logistic Regression最常见的应用场景就是预测概率。比如知道一个人的 年龄、性别、血压、胆固醇水平、体重，想知道这个人患心脏病的概率。

##### 参数及说明

`class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)`

*  **参数**

>**penalty：**惩罚项。
str类型，默认为l2。newton-cg、sag和lbfgs求解算法只支持L2规范,L2假设的模型参数满足高斯分布。
l1:L1规范假设的是模型的参数满足拉普拉斯分布.

>**dual：**对偶或原始方法，bool类型，默认为False。对偶方法只用在求解线性多核(liblinear)的L2惩罚项上。当样本数量>样本特征的时候，dual通常设置为False。

>**tol：**停止求解的标准，float类型，默认为1e-4。就是求解到多少的时候，停止，认为已经求出最优解。

>**c：**正则化系数λ的倒数，float类型，默认为1.0。必须是正浮点型数。像SVM一样，越小的数值表示越强的正则化。

>**fit_intercept：**是否存在截距或偏差，bool类型，默认为True。

>**intercept_scaling：**仅在正则化项为”liblinear”，且fit_intercept设置为True时有用。float类型，默认为1。

>**class_ weight：**用于标示分类模型中各种类型的权重，可以是一个字典或者’balanced’字符串，默认为不输入，也就是不考虑权重，即为None。如果选择输入的话，可以选择balanced让类库自己计算类型权重，或者自己输入各个类型的权重。当class_ weight为balanced时，类权重计算方法如下：n_samples / (n_classes * np.bincount(y))。n_samples为样本数，n_classes为类别数量，np.bincount(y)会输出每个类的样本数。其实这个数据平衡的问题我们有专门的解决办法：重采样。

>**random_state：**随机数种子，int类型，可选参数，默认为无，仅在正则化优化算法为sag,liblinear时有用。

>**solver：**优化算法选择参数。
   *  liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。
   *  lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
   *  newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。只用于L2。
   *  sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。只用于L2。
   *  saga：线性收敛的随机优化算法的的变重。只用于L2。

>**max_iter：**算法收敛最大迭代次数，int类型，默认为10。仅在正则化优化算法为newton-cg, sag和lbfgs才有用，算法收敛的最大迭代次数。

>**multi_class：**分类方式选择参数，str类型，可选参数为ovr和multinomial，默认为ovr。ovr即one-vs-rest(OvR)，而multinomial即many-vs-many(MvM)。如果是二元逻辑回归，ovr和multinomial并没有任何区别，区别主要在多元逻辑回归上。

>**verbose：**日志冗长度，int类型。默认为0。就是不输出训练过程，1的时候偶尔输出结果，大于1，对于每个子模型都输出。

>**warm_start：**热启动参数，bool类型。默认为False。如果为True，则下一次训练是以追加树的形式进行（重新使用上一次的调用作为初始化）。

>**n_jobs：**并行数。int类型，默认为1。为-1的时候，用所有CPU的内核运行程序。

*  **方法**

>decision_function(X): 返回样本距离超平面的距离。

>desify():将参数转换为np.ndarray形式输出。

>fit(X,y,sample_weight=None):训练。

>get_params(deep=True):获得参数。

>predict(X):预测。

>predict_log_proba(X):返回样本对于每一类的对数概率。

>predict_proba(X):返回样本对于每一类的概率。

>score(X, y[, sample_weight]):返回给出的测试数据和标签的平均准确率。

>set_params(**params):改变参数。

>sparsify():将参数转换为稀疏的格式。

##### **python/pyspark**样例代码

`from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
X, y = load_iris(return_X_y=True)
clf = LogisticRegression(multi_class='ovr',solver='liblinear',penalty='l2',C=1.0,random_state=0).fit(X, y)
clf.predict(X[:2, :])
clf.predict_proba(X[:2, :])
clf.score(X, y)`