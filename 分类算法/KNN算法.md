

<h3>最近邻分类</h3>

##### 简介

  kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 kNN方法在类别决策时，只与极少量的相邻样本有关。由于kNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，kNN方法较其他方法更为适合。

##### 理论知识

KNN分类算法的实现过程如下：
1）计算测试数据与各个训练数据之间的距离； 
2）按照距离的递增关系进行排序； 
3）选取距离最小的K个点； 
4）确定前K个点所在类别的出现频率； 
5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。

算法关键：
(1)数据的所有特征都要做可比较的量化
若是数据特征中存在非数值的类型，必须采取手段将其量化为数值。例如样本特征中包含颜色，可通过将颜色转换为灰度值来实现距离计算。
(2)样本特征要做归一化处理
样本有多个参数，每一个参数都有自己的定义域和取值范围，他们对距离计算的影响不一样，如取值较大的影响力会盖过取值较小的参数。所以样本参数必须做一些scale处理，最简单的方式就是所有特征的数值都采取归一化处理。
归一化方法有：

>min-max标准化（Min-Max Normalization）：![b1](/uploads/0c7ce23fc218b413c46b041f4c421cea/b1.png)

>Z-score标准化（Z-score standardization）：![b2](/uploads/cadd54b56d4b71b2a9b9bc07f876b39c/b2.png)，其中 μ 为所有样本数据的均值，σ为所有样本数据的标准差。

(3)需要一个距离函数以计算两个样本之间的距离
距离的定义：欧氏距离、余弦距离、汉明距离、曼哈顿距离等，一般选欧氏距离作为距离度量，但是这是只适用于连续变量。在文本分类这种非连续变量情况下，汉明距离可以用来作为度量。通常情况下，如果运用一些特殊的算法来计算度量的话，K近邻分类精度可显著提高，如运用大边缘最近邻法或者近邻成分分析法。
具体计算公式如下：
>1.欧式距离： 即两点间的空间距离，为两点向量差的L2范数。两个n维向量A（x11,x12,...,x1n）和B（x21，x22,...,x2n）间的欧氏距离为：![d2](/uploads/c4e696487d4e0a96ba68b276e452c96f/d2.gif)

>2.余弦距离：余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。公式如下：![d5](/uploads/8c66a695939671d3ac68d324654182e3/d5.png)

>3.汉明距离：汉明距离定义：两个等长字符串s1和s2之间的汉明距离定义为将其中一个变为另外一个所需要的最小替换次数。如字符串“1111”与“1001”之间的汉明距离为2。也就是指两个字符串的接近程度。

>4.曼哈顿距离：即城市街区距离，为两点向量差的L1范数。两个n维向量A（x11,x12,...,x1n）和B（x21，x22,...,x2n）间的曼哈顿距离为：![d1](/uploads/bf5bc05dffca965ebeabd6cb3f2d80c8/d1.gif)

>5.闵可夫斯基距离：相对于Lp范数，是一组距离的定义。两个n维向量A（x11,x12,...,x1n）和B（x21，x22,...,x2n）间的闵可夫斯基距离的定义为：![d4](/uploads/4240874fbbe731e0e3ec4de2465b8a54/d4.gif)

>6.切比雪夫距离：即最大的维度内距离，为两点向量差的无穷范数。两个n维向量A（x11,x12,...,x1n）和B（x21，x22,...,x2n）间的切比雪夫距离为：![d3](/uploads/08027f2beb5f8817d46a242df5e19b1f/d3.gif)
或另一种等价形式：![d3_1](/uploads/dd25c45e841a9386244b840cba4b65a6/d3_1.gif)

(4)确定K的值
K值选的太大易引起欠拟合，太小容易过拟合。交叉验证确定K值。

##### 应用场景

主要应用领域有文本分类、聚类分析、预测分析、模式识别、图像处理。KNN算法不仅可以用于分类，还可以用于预测。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成反比。适合对多分类问题及稀有事件进行分类。

##### 参数及说明

`class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)`

*  **参数**

>**n_neighbors：**int，optional(default = 5)
默认情况下kneighbors查询使用的邻居数。就是k-NN的k的值，选取最近的k个点。

>**weights：**str或callable，可选(默认=‘uniform’)
默认是uniform，参数可以是uniform、distance，也可以是用户自己定义的函数。uniform是均等的权重，就说所有的邻近点的权重都是相等的。distance是不均等的权重，距离近的点比距离远的点的影响大。用户自定义的函数，接收距离的数组，返回一组维数相同的权重。

>**algorithm：**{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选
快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索，brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。ball tree是为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。

>**leaf_size：**int，optional(默认值= 30)
默认是30，这个是构造的kd树和ball树的大小。这个值的设置会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需要根据问题的性质选择最优的大小。

>**p：**整数，可选(默认= 2)
距离度量公式。在上小结，我们使用欧氏距离公式进行距离度量。除此之外，还有其他的度量方法，例如曼哈顿距离。这个参数默认为2，也就是默认使用欧式距离公式进行距离度量。也可以设置为1，使用曼哈顿距离公式进行距离度量。

>**metric：**字符串或可调用，用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。

>**metric_params：**dict，optional(默认=None)，距离公式的其他关键参数，这个可以不管，使用默认的None即可。

>**n_jobs：**int或None，可选(默认=None)。并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。

*  **方法**

>fit(X, y)：使用X作为训练数据，y作为目标值（类似于标签）来拟合模型。

>get_params([deep])：获取估值器的参数。

>kneighbors([X, n_neighbors, return_distance])：查找一个或几个点的K个邻居。

>kneighbors_graph([X, n_neighbors, mode])：计算在X数组中每个点的k邻居的（权重）图。

>predict(X)：给提供的数据预测对应的标签。

>predict_proba(X)：返回测试数据X的概率估值。

>score(X, y[, sample_weight])：返回给定测试数据和标签的平均准确值。

>set_params(**params)：设置估值器的参数。


##### **python/pyspark**样例代码

`from sklearn.neighbors import KNeighborsClassifier

X = [``[0], [1], [2], [3]``]

y = [0, 0, 1, 1]

neigh = KNeighborsClassifier(n_neighbors=3,algorithm=auto,weights="uniform",leaf_size=30)

neigh.fit(X, y)

neigh.predict([``[1.1]``])

neigh.predict_proba([``[0.9]``])`