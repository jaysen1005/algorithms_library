<h1>算法库</h1>

<h2>分类算法</h2>

所谓分类(Classification)，就是按照某种标准给对象贴标签(label)，再根据标签来区分归类；分类作为一种监督学习方法，它的目标在于通过已有数据的确定类别，学习得到一个分类函数或分类模型(也常常称作分类器)，该模型能把数据库中的数据项映射到给定类别中的某一个类中。简单的说，就是我们在进行分类前，得到的数据已经标示了数据所属的类别，分类的目标就是得到一个分类的标准，使得我们能够更好的把不同类别的数据区分出来。

<h3>支持向量机分类</h3>

##### 简介

支持向量机（Support Vector Machine，简称SVM），是机器学习中运用较为广泛的一种的算法，在神经网络出现之前，应用十分广泛。SVM算法是一种二分类算法，通过构建超平面函数，来进行样本分类。

##### 理论知识

对于二元分类问题，如果存在一个分隔超平面能够将不同类别的数据完美的分隔开（即两类数据正好完全落在超平面的两侧），则称其为线性可分。反之，如果不存在这样的超平面，则称其为线性不可分。所谓超平面，是指能够将n维空间划分为两部分的分隔面，其形如w^T x+b=0。简单来说，对于二维空间（指数据集有两个特征），对应的超平面就是一条直线；对于三维空间（指数据集拥有三个特征），对应的超平面就是一个平面。可以依次类推到n维空间。SVM的目标就是找到这样的一个超平面，使得不同类别的数据能够落在超平面的两侧。
如何计算最优超平面：
>1. 首先根据算法思想："找到具有最小间隔的样本点，然后拟合出一个到这些样本点距离和最大的线段/平面。" 写出目标函数：![s](/uploads/0b9c98bbd1105eb3604ec36cca5b75bc/s.png)。
该式子的解就是待求的回归系数。然而，这是一个嵌套优化问题，非常难进行直接优化求解。为了解这个式子，还需要以下步骤。

>2. 不去计算内层的min优化，而是将距离值界定到一个范围 - 大于1，即最近的样本点，也即支持向量到超平面的距离为1。下图可以清楚表示这个意思：![s2](/uploads/c7f1bde5e30c5a18b67988c80b4a5637/s2.png)。去掉min操作，代之以界定：label * (wTx + b) >= 1。

>3. 这样得到的式子就是一个带不等式的优化问题，可以采用拉格朗日乘子法(KKT条件)去求解。推导结果为：![s3](/uploads/f43682bd8e74065d961e7ec6da932398/s3.png)。另外，可加入松弛系数 C，用于控制 "最大化间隔" 和"保证大部分点的函数间隔小于1.0" 这两个目标的权重。将 α >= 0 条件改为 C >= α >= 0 即可。α 是用于求解过程中的一个向量，它和要求的结果回归系数是一一对应的关系。将其中的 α 解出后，便可依据如下两式子(均为推导过程中出现的式子)进行转换得到回归系数：![s4](/uploads/2afd225a9e7e1bbf6b56d83be5cfe0a6/s4.png)，![s5](/uploads/c6ef26f396d671314b00cc60fcca3ef7/s5.png)。

##### 应用场景

SVM支持向量机，主要用于解决模式识别领域中的数据分类问题，属于有监督学习算法的一种。SVM要解决的问题可以用一个经典的二分类问题加以描述，在模式识别领域称为线性可分问题。

##### 参数及说明

`class sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)`

*  **参数**

>**C：**错误的惩罚系数,(0,1)之间的double类型。默认值是1.0。

>**kernel：**核函数，默认是rbf，可以是‘linear’（线性核函数）, ‘poly’（多项式核函数）, ‘rbf’（径像核函数/高斯核函数）, ‘sigmoid’（sigmoid核函数）, ‘precomputed’（核矩阵）。

>**degree：**多项式poly函数的维度，默认是3，选择其他核函数时会被忽略。

>**gamma：**‘rbf’,‘poly’ 和‘sigmoid’的核函数参数。默认是’auto’，则会选择1/n_features。

> **coef0：**核函数的常数项。对于‘poly’和 ‘sigmoid’有用。

>**probability：**bool型。是否采用概率估计。默认为False。

>**shrinking：**bool型。是否采用启发式收缩方式，默认为true。

> **tol：**停止训练的误差值大小，默认为1e-3。

>**cache_size：**核函数cache缓存大小，默认为200。

>**class_weight：**类别的权重，字典形式传递。

>**verbose：bool型。是否启用详细输出。默认为False。

>**max_iter：**最大迭代次数。-1为无限制。

>**decision_function_shape：**‘ovo’, ‘ovr’ or None, default=None。

>**random_state：**伪随机数发生器的种子,在混洗数据时用于概率估计。int型参数，默认为None。

*  **方法**

>decision_function(X):获取数据集X到分离超平面的距离。

>fit(X, y):在数据集(X,y)上使用SVM模型。

>get_params([deep]):获取模型的参数。

>predict(X):预测数据值X的标签。

>score(X,y):返回给定测试集和对应标签的平均准确率。


##### **python/pyspark**样例代码

`import numpy as np`

`X = np.array([``[-1, -1], [-2, -1], [1, 1], [2, 1]``])`

`y = np.array([1, 1, 2, 2])`

`from sklearn.svm import SVC`

`clf = SVC(gamma='auto')`

`clf.fit(X, y)`

`clf.predict([[-0.8, -1]])`