

<h3>ARIMA</h3>

##### 简介

　　ARIMA模型（AutoregressiveIntegratedMovingAverage model），差分整合移动平均自回归模型，又称整合移动平均自回归模型（移动也可称作滑动），时间序列预测分析方法之一。ARIMA（p，d，q）中，AR是"自回归"，p为自回归项数；MA为"滑动平均"，q为滑动平均项数，d为使之成为平稳序列所做的差分次数（阶数）。“差分”一词虽未出现在ARIMA的英文名称中，却是关键步骤。ARIMA（p，d，q）模型是ARMA（p，q）模型的扩展。ARIMA（p，d，q）模型可以表示为：![t1](/uploads/8150f4403c49bddd66181e01d3e816d7/t1.png)。其中L是滞后算子（Lag operator），![t2](/uploads/a58d707b3d2159e2d7a05fada6b8f1a0/t2.png)。

##### 理论知识

ARIMA模型建立流程如下：
>1.根据时间序列的散点图、自相关函数和偏自相关函数图识别其平稳性。
*  自相关函数ACF(autocorrelation function)：
自相关函数ACF描述的是时间序列观测值与其过去的观测值之间的线性相关性。计算公式如下：![t3](/uploads/3afb8fdbb6e197a187c8c1b387021baf/t3.png)。其中k代表滞后期数，如果k=2，则代表yt和yt-2。
*  偏自相关函数PACF(partial autocorrelation function)：
偏自相关函数PACF描述的是在给定中间观测值的条件下，时间序列观测值预期过去的观测值之间的线性相关性。
举个简单的例子，假设k=3，那么我们描述的是yt和yt-3之间的相关性，但是这个相关性还受到yt-1和yt-2的影响。PACF剔除了这个影响，而ACF包含这个影响。

>2.对非平稳的时间序列数据进行平稳化处理。直到处理后的自相关函数和偏自相关函数的数值非显著非零。
平稳性就是要求经由样本时间序列所得到的拟合曲线，在未来的一段时间内仍能顺着现有状态“惯性”地延续下去；序列的均值和方差不发生明显变化；一般采用差分法对非平稳序列进行平稳化处理。                           

>3.根据所识别出来的特征建立相应的时间序列模型。平稳化处理后，若偏自相关函数是截尾的，而自相关函数是拖尾的，则建立AR模型；若偏自相关函数是拖尾的，而自相关函数是截尾的，则建立MA模型；若偏自相关函数和自相关函数均是拖尾的，则序列适合ARMA模型。                                                                     

>**拖尾**指序列以指数率单调递减或震荡衰减，而截尾指序列从某个时点变得非常小：
![t4](/uploads/db780124f33a2e0b37d87920135916f1/t4.png)                                  
出现以下情况，通常视为(偏)自相关系数d阶截尾：
1）在最初的d阶明显大于2倍标准差范围。
2）之后几乎95%的(偏)自相关系数都落在2倍标准差范围以内。
3）且由非零自相关系数衰减为在零附近小值波动的过程非常突然。                                      
![t5](/uploads/56b73d29a141485160bf9e0a5b0779ca/t5.png)                                  
出现以下情况，通常视为(偏)自相关系数拖尾：
1）如果有超过5%的样本(偏)自相关系数都落入2倍标准差范围之外
2）或者是由显著非0的(偏)自相关系数衰减为小值波动的过程比较缓慢或非常连续。
![t6](/uploads/ddeb511a2c37a5f2b354cb6d35fb4081/t6.png)                                   
**p，q阶数的确定：**
根据刚才判定截尾和拖尾的准则，p，q的确定基于如下的规则：                                           
![t7](/uploads/10d57fe0437238b75644f8ab636942c0/t7.png)                                  
根据不同的截尾和拖尾的情况，我们可以选择AR模型，也可以选择MA模型，当然也可以选择ARIMA模型。

>4.参数估计，检验是否具有统计意义。

>　通过拖尾和截尾对模型进行定阶的方法，往往具有很强的主观性。在相同的预测误差情况下，根据奥斯卡姆剃刀准则，模型越小是越好的。那么，平衡预测误差和参数个数，我们可以根据信息准则函数法，来确定模型的阶数。预测误差通常用平方误差即残差平方和来表示。

>　常用的信息准则函数法有下面几种：

>*  AIC准则
AIC准则全称为全称是最小化信息量准则（Akaike Information Criterion），计算公式如下：AIC = =2 *（模型参数的个数）-2ln（模型的极大似然函数）
>*  BIC准则
AIC准则存在一定的不足之处。当样本容量很大时，在AIC准则中拟合误差提供的信息就要受到样本容量的放大，而参数个数的惩罚因子却和样本容量没关系（一直是2），因此当样本容量很大时，使用AIC准则选择的模型不收敛与真实模型，它通常比真实模型所含的未知参数个数要多。BIC（Bayesian InformationCriterion）贝叶斯信息准则弥补了AIC的不足，计算公式如下：BIC = ln(n) * (模型中参数的个数) – 2ln(模型的极大似然函数值)，n是样本容量

>5.假设检验，判断（诊断）残差序列是否为白噪声序列。

>   这里的模型检验主要有两个：
>  1）检验参数估计的显著性（t检验）。
>  2）检验残差序列的随机性，即残差之间是独立的。
>  残差序列的随机性可以通过自相关函数法来检验，即做残差的自相关函数图。

>6.利用已通过检验的模型进行预测。

##### 应用场景

主要应用领域有文本分类、聚类分析、预测分析、模式识别、图像处理。KNN算法不仅可以用于分类，还可以用于预测。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成反比。适合对多分类问题及稀有事件进行分类。

##### 参数及说明

`class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)`

*  **参数**

>**n_neighbors：**int，optional(default = 5)
默认情况下kneighbors查询使用的邻居数。就是k-NN的k的值，选取最近的k个点。

>**weights：**str或callable，可选(默认=‘uniform’)
默认是uniform，参数可以是uniform、distance，也可以是用户自己定义的函数。uniform是均等的权重，就说所有的邻近点的权重都是相等的。distance是不均等的权重，距离近的点比距离远的点的影响大。用户自定义的函数，接收距离的数组，返回一组维数相同的权重。

>**algorithm：**{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选
快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索，brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。ball tree是为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。

>**leaf_size：**int，optional(默认值= 30)
默认是30，这个是构造的kd树和ball树的大小。这个值的设置会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需要根据问题的性质选择最优的大小。

>**p：**整数，可选(默认= 2)
距离度量公式。在上小结，我们使用欧氏距离公式进行距离度量。除此之外，还有其他的度量方法，例如曼哈顿距离。这个参数默认为2，也就是默认使用欧式距离公式进行距离度量。也可以设置为1，使用曼哈顿距离公式进行距离度量。

>**metric：**字符串或可调用，用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。

>**metric_params：**dict，optional(默认=None)，距离公式的其他关键参数，这个可以不管，使用默认的None即可。

>**n_jobs：**int或None，可选(默认=None)。并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。

*  **方法**

>fit(X, y)：使用X作为训练数据，y作为目标值（类似于标签）来拟合模型。

>get_params([deep])：获取估值器的参数。

>kneighbors([X, n_neighbors, return_distance])：查找一个或几个点的K个邻居。

>kneighbors_graph([X, n_neighbors, mode])：计算在X数组中每个点的k邻居的（权重）图。

>predict(X)：给提供的数据预测对应的标签。

>predict_proba(X)：返回测试数据X的概率估值。

>score(X, y[, sample_weight])：返回给定测试数据和标签的平均准确值。

>set_params(**params)：设置估值器的参数。


##### **python/pyspark**样例代码

`from sklearn.neighbors import KNeighborsClassifier
X = [``[0], [1], [2], [3]``]
y = [0, 0, 1, 1]
neigh = KNeighborsClassifier(n_neighbors=3,algorithm=auto,weights="uniform",leaf_size=30)
neigh.fit(X, y)
neigh.predict([``[1.1]``])
neigh.predict_proba([``[0.9]``])`