

<h3>主成分分析</h3>

##### 简介

　　主成分分析（Principal Component Analysis，PCA）， 是一种统计方法。通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分。

##### 理论知识

　　主成分分析的原理是设法将原来变量重新组合成一组新的相互无关的几个综合变量，同时根据实际需要从中可以取出几个较少的综合变量尽可能多地反映原来变量的信息的统计方法叫做主成分分析或称主分量分析，也是数学上处理降维的一种方法。主成分分析是设法将原来众多具有一定相关性（比如P个指标），重新组合成一组新的互相无关的综合指标来代替原来的指标。通常数学上的处理就是将原来P个指标作线性组合，作为新的综合指标。最经典的做法就是用F1（选取的第一个线性组合，即第一个综合指标）的方差来表达，即Va（rF1）越大，表示F1包含的信息越多。因此在所有的线性组合中选取的F1应该是方差最大的，故称F1为第一主成分。如果第一主成分不足以代表原来P个指标的信息，再考虑选取F2即选第二个线性组合，为了有效地反映原来信息，F1已有的信息就不需要再出现再F2中，用数学语言表达就是要求Cov（F1,F2）=0，则称F2为第二主成分，依此类推可以构造出第三、第四，……，第P个主成分。

　　主成分分析法的计算步骤：                                                                  
　　1、原始指标数据的标准化采集p 维随机向量x = (x1,X2,...,Xp)^T )n 个样品xi = (xi1,xi2,...,xip)^T ，i=1,2,…,n，n＞p，构造样本阵，对样本阵元进行如下标准化变换：![pc1](/uploads/597a9889d101a67d8d4e226545979984/pc1.png)，其中![pc2](/uploads/2ebe2acc9adc6bab67ad5f2a4c642e5e/pc2.png)，得标准化阵Z。                         
　　2、对标准化阵Z 求相关系数矩阵                                                             
![pc3](/uploads/901528d7d8a9872d19aab49596b5df95/pc3.png)，其中，![pc4](/uploads/dd663caebdcc6aebdf3932d53af76686/pc4.png)。                                     
　　3、解样本相关矩阵R 的特征方程![pc5](/uploads/3e57ffd36e259912136f19912664a1c0/pc5.png)得p 个特征根,确定主成分                                                                                                                                                       
按![pc6](/uploads/c91e3b6716f5f275a7bbaa514590dbc5/pc6.png)确定m 值，使信息的利用率达85%以上，对每个λj, j=1,2,...,m, 解方程组Rb = λjb得单位特征向量![pc7](/uploads/05f31c4e8ca8d95b47928cf8ec1ed30f/pc7.png)。                                       
　　4、将标准化后的指标变量转换为主成分                                                         
![pc8](/uploads/99c4ee0c91a0577037d21cfb03b6d2d4/pc8.png)。U1称为第一主成分,U2 称为第二主成分,…,Up 称为第p 主成分。                                                                      
　　5、对m 个主成分进行综合评价                                                              
 对m 个主成分进行加权求和，即得最终评价值，权数为每个主成分的方差贡献率。

##### 应用场景

PCA的主要适用场景：                                                                         
（1）非监督式的数据集                                                                        
它是一种非监督式的降维方法，因此适用于不带有标签的数据集，对于带有标签的可以采用LDA。                  
（2）根据方差自主控制特征数量                                                                 
最大的主成分的数量会小于或等于特征的数量，即，PCA可以输出全部的特征，具体取决于选择特征中解释的方差比例。  
（3）更少的正则化处理                                                                         
选择较多的主成分将导致更少的平滑，因为能保留很多特征，减少正则化。                                     
（4）数据量较大的数据集                                                                        
数据量大指数据记录多和维度多两种情况，PCA对大型数据集的处理效率高。                                  
（5）数据分布是位于相同平面上，数据中存在线性结构                                                  

##### 参数及说明

`class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False)`

*  **参数**

>**n_components:** int, None or string。降维后的主成成分数量。                              
int 或者 string，缺省时默认为None，所有成分被保留。                                            
赋值为int，比如n_components=1，将把原始数据降到一个维度。                                        
赋值为string，比如n_components='mle'，将自动选取特征个数n，使得满足所要求的方差百分比。

>**copy：** bool。表示是否在运行算法时，将原始训练数据复制一份。若为True，则运行PCA算法后，原始训练数据的值不会有任何改变，因为是在原始数据的副本上进行运算；若为False，则运行PCA算法后，原始训练数据的值会改，因为是在原始数据上进行降维计算。

>**whiten：**bool。白化，缺省时默认为False。

##### **python/pyspark**样例代码

`import numpy as np
from sklearn.decomposition import PCA
X = np.array([``[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]``])
pca = PCA(n_components=2)
pca.fit(X)
print(pca.explained_variance_ratio_)`